{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Notebook: System Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Experiment Summary](#Experiment-Summary)\n",
    "* [Experiment Assumptions](#Experiment-Assumptions)\n",
    "* [Experiment Setup](#Experiment-Setup)\n",
    "* [Analysis 1: Sanity Checks](#Analysis-1:-Sanity-Checks)\n",
    "* [Analysis 2: Correlation Matrix](#Analysis-1:-Correlation-Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Summary \n",
    "\n",
    "The purpose of this notebook is to demonstrate the system's standard metrics, KPIs and goals.\n",
    "\n",
    "# Experiment Assumptions\n",
    "\n",
    "See [assumptions document](../../ASSUMPTIONS.md) for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setup\n",
    "\n",
    "We begin with several experiment-notebook-level preparatory setup operations:\n",
    "\n",
    "* Import relevant dependencies\n",
    "* Import relevant experiment templates\n",
    "* Create copies of experiments\n",
    "* Configure and customize experiments \n",
    "\n",
    "Analysis-specific setup operations are handled in their respective notebook sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the setup module:\n",
    "# * sets up the Python path\n",
    "# * runs shared notebook configuration methods, such as loading IPython modules\n",
    "import setup\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "import experiments.notebooks.visualizations as visualizations\n",
    "from experiments.run import run\n",
    "from experiments.utils import display_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable/disable logging\n",
    "logger = logging.getLogger()\n",
    "logger.disabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import experiment templates\n",
    "import experiments.default_experiment as default_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect experiment template\n",
    "display_code(default_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simulation for each analysis\n",
    "simulation_1 = copy.deepcopy(default_experiment.experiment.simulations[0])\n",
    "simulation_2 = copy.deepcopy(default_experiment.experiment.simulations[0])\n",
    "simulation_3 = copy.deepcopy(default_experiment.experiment.simulations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "# simulation_1.model.initial_state.update({})\n",
    "# simulation_1.model.params.update({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 1: PCV Sanity Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simulation across 4 volatile asset price scenarios to validate PCV states and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis-specific setup\n",
    "simulation_1.model.params.update({\n",
    "    \"volatile_asset_price_process\": [\n",
    "        lambda _run, _timestep: 2_000,\n",
    "        lambda _run, timestep: 2_000 if timestep < 365 / 4 else (1_000 if timestep < 365 * 3/4 else 2_000),\n",
    "        lambda _run, timestep: 2_000 * (1 + timestep * 0.2 / 365),\n",
    "        lambda _run, timestep: 2_000 * (1 - timestep * 0.2 / 365),\n",
    "    ],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment execution\n",
    "df, exceptions = run(simulation_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Post-processing and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df.plot(y='volatile_asset_price', color='subset')\n",
    "\n",
    "fig.update_xaxes(title='Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import itertools\n",
    "from experiments.notebooks.visualizations.plotly_theme import cadlabs_colorway_sequence\n",
    "color_cycle = itertools.cycle(cadlabs_colorway_sequence)\n",
    "\n",
    "\n",
    "fig = make_subplots(rows=5, cols=len(df.subset.unique()), shared_yaxes=True)\n",
    "\n",
    "for subset in df.subset.unique():\n",
    "    df_plot = df.query('subset == @subset')\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.timestamp,\n",
    "            y=df_plot.volatile_asset_price,\n",
    "            name=\"Volatile Asset Price\",\n",
    "            line=dict(color=cadlabs_colorway_sequence[0]),\n",
    "            showlegend=(True if subset == 0 else False),\n",
    "        ),\n",
    "        row=1, col=subset+1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.timestamp,\n",
    "            y=df_plot.total_pcv,\n",
    "            name=\"Total PCV\",\n",
    "            line=dict(color=cadlabs_colorway_sequence[1]),\n",
    "            showlegend=(True if subset == 0 else False),\n",
    "        ),\n",
    "        row=2, col=subset+1\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.timestamp,\n",
    "            y=df_plot.collateralization_ratio,\n",
    "            name=\"Collateralization Ratio\",\n",
    "            line=dict(color=cadlabs_colorway_sequence[2]),\n",
    "            showlegend=(True if subset == 0 else False),\n",
    "        ),\n",
    "        row=3, col=subset+1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.timestamp,\n",
    "            y=df_plot.total_volatile_asset_pcv,\n",
    "            name=\"Volatile Asset PCV\",\n",
    "            line=dict(color=cadlabs_colorway_sequence[3]),\n",
    "            showlegend=(True if subset == 0 else False),\n",
    "            stackgroup='one',\n",
    "        ),\n",
    "        row=4, col=subset+1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.timestamp,\n",
    "            y=df_plot.total_stable_asset_pcv,\n",
    "            name=\"Stable Asset PCV\",\n",
    "            line=dict(color=cadlabs_colorway_sequence[4]),\n",
    "            showlegend=(True if subset == 0 else False),\n",
    "            stackgroup='one',\n",
    "        ),\n",
    "        row=4, col=subset+1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.timestamp,\n",
    "            y=df_plot.liquidity_pool_tvl,\n",
    "            name=\"Liquidity Pool TVL\",\n",
    "            line=dict(color=cadlabs_colorway_sequence[4]),\n",
    "            showlegend=(True if subset == 0 else False),\n",
    "        ),\n",
    "        row=5, col=subset+1\n",
    "    )\n",
    "\n",
    "\n",
    "fig.update_layout(height=1000, title_text=\"PCV Sanity Checks\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 2: Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analysis-specific setup\n",
    "simulation_2.model.params.update({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment execution\n",
    "df, exceptions = run(simulation_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# corr = df[[\n",
    "#     \"volatile_asset_price\",\n",
    "#     \"total_pcv\",\n",
    "#     \"collateralization_ratio\",\n",
    "#     \"total_stable_asset_pcv\",\n",
    "#     \"liquidity_pool_tvl\"\n",
    "# ]].corr()\n",
    "\n",
    "# _ = sns.heatmap(corr,\n",
    "#             xticklabels=corr.columns.values,\n",
    "#             yticklabels=corr.columns.values)\n",
    "\n",
    "# import plotly.plotly as py\n",
    "# from plotly.graph_objs import *\n",
    "\n",
    "# trace1 = {\n",
    "#   \"type\": \"heatmap\", \n",
    "#   \"x\": [\"fp_time\", \"uid\", \"revenue\", \"num_trans\", \"act_days\", \"lt\", \"delt_pmnt_log\"], \n",
    "#   \"y\": [\"fp_time\", \"uid\", \"revenue\", \"num_trans\", \"act_days\", \"lt\", \"delt_pmnt_log\"], \n",
    "#   \"z\": [],\n",
    "# }\n",
    "\n",
    "# data = Data([trace1])\n",
    "# layout = {\"title\": \"Features Correlation Matrix\"}\n",
    "# fig = Figure(data=data, layout=layout)\n",
    "# plot_url = py.plot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCV at Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import lt, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_3.runs = 20\n",
    "\n",
    "parameter_overrides = {\n",
    "    \"target_rebalancing_condition\": [gt, lt], # Simulate decrease and increase of stable PCV\n",
    "    \"target_stable_pcv_ratio\": [0.2, 0.5], # Simulate decrease and increase of stable PCV\n",
    "    \"rebalancing_period\": [int(365 / 4)],\n",
    "}\n",
    "\n",
    "simulation_3.model.params.update(parameter_overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Experiment execution\n",
    "df, exceptions = run(simulation_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(y=\"volatile_asset_price\", color=\"run\", facet_row=\"subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(y=\"total_pcv\", color=\"run\", facet_row=\"subset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorized VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_VaR_run(df, n_run, alpha, n_timesteps, t_start, t_end):\n",
    "    pcv_ret = df.query('run==@n_run and (timestep > @t_start and timestep <= @t_end)')['total_pcv'].pct_change()\n",
    "    pcv_final_val = df.query('run==@n_run')['total_pcv'].iloc[-1]\n",
    "    q = pcv_ret.quantile(1-alpha)\n",
    "    # see https://www0.gsb.columbia.edu/faculty/pglasserman/B6014/var-d.pdf\n",
    "    # for n-day var simplifying assumption which allows for generalization\n",
    "    VaR_n = abs(pcv_final_val * q)*np.sqrt(n_timesteps)\n",
    "    \n",
    "    return VaR_n, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_VaR_subset(df, n_subset, alpha, n_timesteps, t_start, t_end):\n",
    "    \n",
    "    VAR = []\n",
    "    \n",
    "    df_ = df.query(\"subset==@n_subset\")\n",
    "    for run in df_['run'].value_counts().index:\n",
    "        var, q = calculate_VaR_run(df_, run, alpha, n_timesteps, t_start, t_end)\n",
    "        \n",
    "        VAR.append((n_subset, var, q))\n",
    "    \n",
    "    return pd.DataFrame(VAR, columns=[x+'_'+str(n_timesteps) for x in ['subset', 'VaR', 'q']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_VaR(df, alpha, n_timesteps, t_start, t_end):\n",
    "    \n",
    "    L = []    \n",
    "    for subset in df['subset'].value_counts().index:\n",
    "        VaR_subset = calculate_VaR_subset(df, subset, alpha, n_timesteps, t_start, t_end)\n",
    "        L.append(VaR_subset)\n",
    "        \n",
    "    return pd.concat(L, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_VaR_n(df, alpha, timestep_range, t_start, t_end):\n",
    "    U, L = [], []\n",
    "    \n",
    "    for t in range(timestep_range):\n",
    "        L.append(calculate_VaR(df, 0.95, t+1, t_start, t_end))\n",
    "        U.append(t+1)\n",
    "        \n",
    "    return dict(zip(U, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_VaR_summary_stats(df, n_timesteps):\n",
    "    L = []\n",
    "    colnames = []\n",
    "    for subset in df['subset'+'_'+str(n_timesteps)].value_counts().index:\n",
    "        L.append(df.query('subset'+'_'+str(n_timesteps)+'==@subset').describe())\n",
    "        colnames += [colname+'_'+str(subset) for colname in df.columns]\n",
    "    \n",
    "    VAR_info = pd.concat(L, axis=1)\n",
    "    VAR_info.columns = colnames\n",
    "    VAR_info = VAR_info.drop(index=['count'])\n",
    "    return VAR_info\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate 1-day vectorized VaR for all simulation outputs\n",
    "# set window bounds, whole simulation for simplicity\n",
    "t_start = 0\n",
    "t_end = 365\n",
    "alpha = 0.95\n",
    "max_day_VaR = 10\n",
    "VAR_df = calculate_VaR_n(df, alpha, max_day_VaR, t_start, t_end)\n",
    "\n",
    "# NOTE: create rolling window by further vectorizing VaR calculation by iterating over start and end time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_1_stats = calculate_VaR_summary_stats(VAR_df[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_10_stats = calculate_VaR_summary_stats(VAR_df[10], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at 1 day VaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_1_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: iterate the print statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Over the simulation, for parameters in the sweep corresponding to subset 0,',\n",
    "      np.round(VAR_1_stats['VaR_1_0'].loc['mean'], 2), 'USD is the mean 1-Day PCVaR at 95% over the 20 runs performed, with associated',\n",
    "      np.round(VAR_1_stats['q_1_0'].loc['mean'], 4), '% average 5% quantile percentile loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Over the simulation, for parameters in the sweep corresponding to subset 1,',\n",
    "      np.round(VAR_1_stats['VaR_1_1'].loc['mean'], 2), 'USD is the mean 1-Day PCVaR at 95% over the 20 runs performed, with associated',\n",
    "      np.round(VAR_1_stats['q_1_1'].loc['mean'], 4), '% average 5% quantile percentile loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at 10 day VaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_10_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Over the simulation, for parameters in the sweep corresponding to subset 0,',\n",
    "      np.round(VAR_10_stats['VaR_10_0'].loc['mean'], 2), 'USD is the mean 10-Day PCVaR at 95% over the 20 runs performed, with associated',\n",
    "      np.round(VAR_10_stats['q_10_0'].loc['mean'], 4), '% average 5% quantile percentile loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Over the simulation, for parameters in the sweep corresponding to subset 1,',\n",
    "      np.round(VAR_10_stats['VaR_10_1'].loc['mean'], 2), 'USD is the mean 10-Day PCVaR at 95% over the 20 runs performed, with associated',\n",
    "      np.round(VAR_10_stats['q_10_1'].loc['mean'], 4), '% average 5% quantile percentile loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generic state variable summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_stats_for_simulation_average(df, subset, cols=None):\n",
    "    \n",
    "    df_ = df[cols].groupby(['subset','timestep']).mean().reset_index().query('subset==@subset')\n",
    "    stats_df = df_.describe()\n",
    "    stats_df.loc['skew'] = df_.skew()\n",
    "    stats_df.loc['kurtosis'] = df_.kurtosis()\n",
    "    # TODO: max drawdown & other relevant summary stats here\n",
    "\n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_stats_for_simulation_run(df, subset, run, cols=None):\n",
    "    df_ = df[cols].query('subset==@subset and run==@run')\n",
    "\n",
    "    stats_df = df_.describe()\n",
    "    stats_df.loc['skew'] = df_.skew()\n",
    "    stats_df.loc['kurtosis'] = df_.kurtosis()\n",
    "    # TODO: max drawdown & other relevant summary stats here\n",
    "\n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass only subset of relevant state variables here\n",
    "get_summary_stats_for_simulation_average(df, 0, cols=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_summary_stats_for_simulation_run(df, 0, 1, cols=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## capital allocation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_fei_deposits(df, fei_cols):\n",
    "    \n",
    "    df_addtl_cols = ['subset', 'run', 'timestep']\n",
    "    \n",
    "    norm_df = df[fei_cols].div(df[fei_cols].sum(axis=1), axis=0)\n",
    "    \n",
    "    return pd.concat([df[df_addtl_cols], norm_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fei_deposit_colnames(col_prefixes, col_suffix):\n",
    "    return [x+col_suffix for x in col_prefixes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_allocations_at_timestep(fei_capital_allocations, fei_cols, run, subset, timestep=-1):\n",
    "    df_ = fei_capital_allocations.query('run==@run and subset==@subset')[fei_cols].iloc[timestep]\n",
    "    df_ = pd.DataFrame(df_).reset_index()\n",
    "    df_.columns = ['index', 'values']\n",
    "    \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_allocations_at_timestep(fei_capital_allocations, fei_cols, subset, timestep=-1):\n",
    "    df_ = fei_capital_allocations.query('subset==@subset')[fei_cols].iloc[timestep]\n",
    "    df_ = pd.DataFrame(df_).reset_index()\n",
    "    df_.columns = ['index', 'values']\n",
    "    \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fei_deposit_prefixes = ['fei_deposit_idle','fei_deposit_liquidity_pool', 'fei_deposit_money_market']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fei_deposit_balances = get_fei_deposit_colnames(fei_deposit_prefixes, '_balance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fei_balance_capital_allocations = normalize_fei_deposits(df, fei_deposit_balances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fei_balance_capital_allocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For one run where sim is driven by stochastic processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fei_balance_capital_allocations.query('run==1').plot(y=fei_deposit_balances, facet_row=\"subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep_allocations_0 = get_allocations_at_timestep(fei_balance_capital_allocations, fei_deposit_balances, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep_allocations_1 = get_allocations_at_timestep(fei_balance_capital_allocations, fei_deposit_balances, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(timestep_allocations_0, values='values', names='index', title='FEI Ecosystem Capital Allocation for Subset 0, run 1')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(timestep_allocations_1, values='values', names='index', title='FEI Ecosystem Capital Allocation for Subset 1, run 1')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average over simulation runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_capital_allocations(fei_capital_allocations):\n",
    "    df_ = fei_balance_capital_allocations.groupby(['subset','timestep']).mean().reset_index()\n",
    "    df_ = df_.drop(columns='run')\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fei_balance_avg_capital_allocations = get_average_capital_allocations(fei_balance_capital_allocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_timestep_allocations_0 = get_avg_allocations_at_timestep(fei_balance_avg_capital_allocations, fei_deposit_balances, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_timestep_allocations_1 = get_avg_allocations_at_timestep(fei_balance_avg_capital_allocations, fei_deposit_balances, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(avg_timestep_allocations_0, values='values', names='index', title='FEI Ecosystem Capital Allocation for Subset 0, averaged over all runs')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(avg_timestep_allocations_1, values='values', names='index', title='FEI Ecosystem Capital Allocation for Subset 1, averaged over all runs')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
