{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Notebook: System Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Experiment Summary](#Experiment-Summary)\n",
    "* [Experiment Assumptions](#Experiment-Assumptions)\n",
    "* [Experiment Setup](#Experiment-Setup)\n",
    "* [Analysis 1: Sanity Checks](#Analysis-1:-Sanity-Checks)\n",
    "* [Analysis 2: Correlation Matrix](#Analysis-1:-Correlation-Matrix)\n",
    "* [Analysis 3: PCV at Risk](#Analysis-3:-PCV-at-Risk)\n",
    "* [Analysis 4: Capital Allocation Metrics](#Analysis-4:-Capital-Allocation-Metrics)\n",
    "* [Assorted Metrics](#Analysis-4:-Assorted-Metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Summary \n",
    "\n",
    "The purpose of this notebook is to demonstrate the system's standard metrics, KPIs and goals.\n",
    "\n",
    "# Experiment Assumptions\n",
    "\n",
    "See [assumptions document](../../ASSUMPTIONS.md) for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setup\n",
    "\n",
    "We begin with several experiment-notebook-level preparatory setup operations:\n",
    "\n",
    "* Import relevant dependencies\n",
    "* Import relevant experiment templates\n",
    "* Create copies of experiments\n",
    "* Configure and customize experiments \n",
    "\n",
    "Analysis-specific setup operations are handled in their respective notebook sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the setup module:\n",
    "# * sets up the Python path\n",
    "# * runs shared notebook configuration methods, such as loading IPython modules\n",
    "import setup\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "import experiments.notebooks.visualizations as visualizations\n",
    "from experiments.run import run\n",
    "from experiments.utils import display_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable/disable logging\n",
    "logger = logging.getLogger()\n",
    "logger.disabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import experiment templates\n",
    "import experiments.default_experiment as default_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect experiment template\n",
    "display_code(default_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simulation for each analysis\n",
    "simulation_1 = copy.deepcopy(default_experiment.experiment.simulations[0])\n",
    "simulation_2 = copy.deepcopy(default_experiment.experiment.simulations[0])\n",
    "simulation_3 = copy.deepcopy(default_experiment.experiment.simulations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "# simulation_1.model.initial_state.update({})\n",
    "# simulation_1.model.params.update({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Analysis Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the FEI Ecosystem Model Risk Analysis is to provide qualitative and quantitative recommendations to surface the most appropriate FEI monetary policy parameter settings across multiple scenarios. The risk analysis includes a cohesive set of model metrics, risk scores and protocol KPIs.\n",
    "\n",
    "The main holistic risk analysis and parameter recommendation tool at our disposal is the FEI Capital Allocation model, where we allow the model to provide risk-weighted target allocations of FEI in all modeled avenues for FEI liquidity - LP, MM, FSD, Idle. These allocation targets will depend on parameter sweeps of FEI's monetary policy levers such as the FEI Savings Rate (see simplified ERD). The highest scoring parameter settings in terms of FEI Capital allocation and associated comparative KPIs will form the quantitative basis for recommendations.\n",
    "\n",
    "It is to be noted that we can analyze KPIs comparatively based on monetary policy parameter settings in <b>Two ways</b> - qualitative/deterministic, and statistically based/stochastic.\n",
    "\n",
    "Since the main driver of volatility in the FEI ecosystem is the Volatile Asset (an abstraction for Ethereum), we can model this volatile asset as Trajectory-based (based on a linear function), or based on a stochastic process such as a Geometric Brownian motion. The application of both settings for the Volatile asset allows us to respectively perform two analysis types.\n",
    "\n",
    "1. Qualitative recommendations can be made as a result of comparative KPI analysis across different monetary policy parameter settings, using a trajectory model for the Volatile Asset. These do NOT involve monte carlo runs. The output of these analyses is to understand the impact on KPIs as a result of monetary policy changes for various final levels of Volatile Asset price. Ex: For a 30% VA price downturn, Stable backing ratio is higher with monetary policy 1 than policy 2, ie: Delta_1,2 Stable Backing ratio > 0, hence policy 1 is recommended.\n",
    "\n",
    "\n",
    "2. Statistically-based recommendations can be made as a result of comparative KPI analysis across different monetary policy parameter settings, using a parameterized stochastic model for the Volatile Asset. These DO involve monte carlo runs. The output of these analyses is to construct a probability distribution for each KPI from which summary statistics can be derived. This allows us to empirically say that for a given parameter setting, KPIs are above or below key thresholds with a certain probability. Ex: Over 1000 simulations, 1-Day PCV at Risk is < 1M USD with a 90% probability with policy 1 and < 1M USD with an 85% probability with policy 2. Additionally, the statistical average PCVaR is lower with policy 1 than with policy 2, ie: Delta_1,2 avg. PCVaR > 0. Hence policy 1 is recommended.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 1: PCV Sanity Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simulation across 4 volatile asset price scenarios to illustrate PCV state evolution. Here, deterministic price trajectories for the Volatile Asset price are used, as opposed to parameterized stochastic processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis-specific setup\n",
    "simulation_1.model.params.update({\n",
    "    \"volatile_asset_price_process\": [\n",
    "        lambda _run, _timestep: 2_000,\n",
    "        lambda _run, timestep: 2_000 if timestep < 365 / 4 else (1_000 if timestep < 365 * 3/4 else 2_000),\n",
    "        lambda _run, timestep: 2_000 * (1 + timestep * 0.2 / 365),\n",
    "        lambda _run, timestep: 2_000 * (1 - timestep * 0.2 / 365),\n",
    "    ],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment execution\n",
    "df, exceptions = run(simulation_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Post-processing and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df.plot(y='volatile_asset_price', color='subset')\n",
    "\n",
    "fig.update_xaxes(title='Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import itertools\n",
    "from experiments.notebooks.visualizations.plotly_theme import cadlabs_colorway_sequence\n",
    "color_cycle = itertools.cycle(cadlabs_colorway_sequence)\n",
    "\n",
    "\n",
    "fig = make_subplots(rows=5, cols=len(df.subset.unique()), shared_yaxes=True)\n",
    "\n",
    "for subset in df.subset.unique():\n",
    "    df_plot = df.query('subset == @subset')\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.timestamp,\n",
    "            y=df_plot.volatile_asset_price,\n",
    "            name=\"Volatile Asset Price\",\n",
    "            line=dict(color=cadlabs_colorway_sequence[0]),\n",
    "            showlegend=(True if subset == 0 else False),\n",
    "        ),\n",
    "        row=1, col=subset+1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.timestamp,\n",
    "            y=df_plot.total_pcv,\n",
    "            name=\"Total PCV\",\n",
    "            line=dict(color=cadlabs_colorway_sequence[1]),\n",
    "            showlegend=(True if subset == 0 else False),\n",
    "        ),\n",
    "        row=2, col=subset+1\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.timestamp,\n",
    "            y=df_plot.collateralization_ratio,\n",
    "            name=\"Collateralization Ratio\",\n",
    "            line=dict(color=cadlabs_colorway_sequence[2]),\n",
    "            showlegend=(True if subset == 0 else False),\n",
    "        ),\n",
    "        row=3, col=subset+1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.timestamp,\n",
    "            y=df_plot.total_volatile_asset_pcv,\n",
    "            name=\"Volatile Asset PCV\",\n",
    "            line=dict(color=cadlabs_colorway_sequence[3]),\n",
    "            showlegend=(True if subset == 0 else False),\n",
    "            stackgroup='one',\n",
    "        ),\n",
    "        row=4, col=subset+1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.timestamp,\n",
    "            y=df_plot.total_stable_asset_pcv,\n",
    "            name=\"Stable Asset PCV\",\n",
    "            line=dict(color=cadlabs_colorway_sequence[4]),\n",
    "            showlegend=(True if subset == 0 else False),\n",
    "            stackgroup='one',\n",
    "        ),\n",
    "        row=4, col=subset+1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.timestamp,\n",
    "            y=df_plot.liquidity_pool_tvl,\n",
    "            name=\"Liquidity Pool TVL\",\n",
    "            line=dict(color=cadlabs_colorway_sequence[4]),\n",
    "            showlegend=(True if subset == 0 else False),\n",
    "        ),\n",
    "        row=5, col=subset+1\n",
    "    )\n",
    "\n",
    "\n",
    "fig.update_layout(height=1000, title_text=\"PCV Sanity Checks\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 2: Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analysis-specific setup\n",
    "simulation_2.model.params.update({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment execution\n",
    "df_2, exceptions = run(simulation_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "variables = [\n",
    "    \"volatile_asset_price\",\n",
    "    \"total_pcv\",\n",
    "    \"collateralization_ratio\",\n",
    "    \"total_stable_asset_pcv\",\n",
    "    \"liquidity_pool_tvl\"\n",
    "]\n",
    "\n",
    "z = df_2[variables].corr().values.tolist()\n",
    "\n",
    "fig = px.imshow(z, x=variables, y=variables, color_continuous_scale='RdBu_r', origin='lower')\n",
    "fig.update_xaxes(side=\"top\")\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "        z=z,\n",
    "        x=variables,\n",
    "        y=variables,\n",
    "        colorscale='Inferno'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 3: PCV at Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import lt, gt\n",
    "\n",
    "\n",
    "simulation_3.runs = 10\n",
    "\n",
    "parameter_overrides = {\n",
    "    \"target_rebalancing_condition\": [gt, lt], # Simulate decrease and increase of stable PCV\n",
    "    \"target_stable_pcv_ratio\": [0.2, 0.5], # Simulate decrease and increase of stable PCV\n",
    "    \"rebalancing_period\": [int(365 / 4)],\n",
    "}\n",
    "\n",
    "simulation_3.model.params.update(parameter_overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment execution\n",
    "df_3, exceptions = run(simulation_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(y=\"volatile_asset_price\", color=\"run\", facet_row=\"subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(y=\"total_pcv\", color=\"run\", facet_row=\"subset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized PCV at Risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis, which is based on a stochastic Volatile Asset Price process with 10 realizations (10 monte carlo runs), we look at two policies (a parameter sweep of size 2) and look at the empirical probability of the PCV at Risk KPI being below a certain threshold, as well as comparatively examine the value of the KPI across both policies to yield a recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_VaR_run(df, n_run, alpha, n_timesteps, t_start, t_end):\n",
    "    pcv_ret = df.query('run==@n_run and (timestep > @t_start and timestep <= @t_end)')['total_pcv'].pct_change()\n",
    "    pcv_final_val = df.query('run==@n_run')['total_pcv'].iloc[-1]\n",
    "    q = pcv_ret.quantile(1-alpha)\n",
    "    # see https://www0.gsb.columbia.edu/faculty/pglasserman/B6014/var-d.pdf\n",
    "    # for n-day var simplifying assumption which allows for generalization\n",
    "    VaR_n = abs(pcv_final_val * q)*np.sqrt(n_timesteps)\n",
    "    \n",
    "    return VaR_n, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_VaR_subset(df, n_subset, alpha, n_timesteps, t_start, t_end):\n",
    "    VAR = []\n",
    "    \n",
    "    df_ = df.query(\"subset==@n_subset\")\n",
    "    for run in df_['run'].value_counts().index:\n",
    "        var, q = calculate_VaR_run(df_, run, alpha, n_timesteps, t_start, t_end)\n",
    "        \n",
    "        VAR.append((n_subset, var, q))\n",
    "    \n",
    "    return pd.DataFrame(VAR, columns=[x+'_'+str(n_timesteps) for x in ['subset', 'VaR', 'q']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_VaR(df, alpha, n_timesteps, t_start, t_end):\n",
    "    L = []\n",
    "    \n",
    "    for subset in df['subset'].value_counts().index:\n",
    "        VaR_subset = calculate_VaR_subset(df, subset, alpha, n_timesteps, t_start, t_end)\n",
    "        L.append(VaR_subset)\n",
    "        \n",
    "    return pd.concat(L, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_VaR_n(df, alpha, timestep_range, t_start, t_end):\n",
    "    U, L = [], []\n",
    "    \n",
    "    for t in range(timestep_range):\n",
    "        L.append(calculate_VaR(df, 0.95, t+1, t_start, t_end))\n",
    "        U.append(t+1)\n",
    "        \n",
    "    return dict(zip(U, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_VaR_summary_stats(df, n_timesteps):\n",
    "    L = []\n",
    "    colnames = []\n",
    "    for subset in df['subset'+'_'+str(n_timesteps)].value_counts().index:\n",
    "        L.append(df.query('subset'+'_'+str(n_timesteps)+'==@subset').describe())\n",
    "        colnames += [colname+'_'+str(subset) for colname in df.columns]\n",
    "    \n",
    "    VAR_info = pd.concat(L, axis=1)\n",
    "    VAR_info.columns = colnames\n",
    "    VAR_info = VAR_info.drop(index=['count'])\n",
    "    return VAR_info\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate 1-day vectorized VaR for all simulation outputs\n",
    "# set window bounds, whole simulation for simplicity\n",
    "t_start = 0\n",
    "t_end = 365\n",
    "alpha = 0.95\n",
    "max_day_VaR = 10\n",
    "VAR_df_dict = calculate_VaR_n(df, alpha, max_day_VaR, t_start, t_end)\n",
    "\n",
    "# NOTE: create rolling window by further vectorizing VaR calculation by iterating over start and end time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_1_stats = calculate_VaR_summary_stats(VAR_df_dict[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_10_stats = calculate_VaR_summary_stats(VAR_df_dict[10], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-day PCV at Risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visualization of PCVaR calculation for one monte carlo run for one Policy (parameter setting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.95\n",
    "pcv_ret = df.query('run==1 and subset==0')['total_pcv'].pct_change()\n",
    "var, q = calculate_VaR_run(df, 1, alpha, 1, 0, 365)\n",
    "\n",
    "\n",
    "fig = px.histogram(pcv_ret, x=\"total_pcv\", title=\"Histogram of PCV Returns for Run 1, Policy 0\")\n",
    "fig.add_vline(x=q)\n",
    "\n",
    "fig.show()\n",
    "print('1-Day PCVar for Run 1, Policy 0 (Subset 0) is', np.round(var,2),\n",
    "      'USD with 5% quantile value', 100*np.round(q, 4),'%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCVaR Summary Statistics across all monte carlo runs for each policy (parameter subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_1_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"1-day average PCV at Risk at 95th quantile for subset 0: \\n {np.round(VAR_1_stats['VaR_1_0'].loc['mean'], 2):,} USD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"1-day average PCV at Risk at 95th quantile for subset 1: \\n {np.round(VAR_1_stats['VaR_1_1'].loc['mean'], 2):,} USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Probability of KPI at Threshold analyisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_VaR_threshold_probability(df, n_timesteps, threshold):\n",
    "    L = []\n",
    "    colnames = []\n",
    "    for subset in df['subset'+'_'+str(n_timesteps)].value_counts().index:\n",
    "        df_ = (df.query('subset'+'_'+str(n_timesteps)+'==@subset')['q_'+str(n_timesteps)] <= threshold).astype(int)\n",
    "        emp_probability = df_.sum()/len(df_)\n",
    "        L.append(emp_probability)\n",
    "        colnames.append('q_'+str(n_timesteps)+'_'+str(subset))\n",
    "    return dict(zip(colnames,L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for how many simulation runs PCV at Risk is < 1% of total PCV\n",
    "quantile_return_threshold = -1e-2\n",
    "q_probabilities = calculate_VaR_threshold_probability(VAR_df_dict[1], 1, Quantile_return_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('For Policy 1, the 1-Day PCV at risk is less than 1% with a', 100*q_probabilities['q_1_0'],'% probability')\n",
    "print('For Policy 2, the 1-Day PCV at risk is less than 1% with a', 100*q_probabilities['q_1_1'],'% probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this analysis, we see that over the 10 monte carlo runs for each policy (each subset), since the probability of PCV at risk being less than 1% of total PCV on any given day is lower for policy 1 than for policy 2, policy 1 does a better job at risk mitigation, hence we recommend policy 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative Average KPI analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_VaR_delta = np.round(VAR_1_stats['VaR_1_0'].loc['mean'], 2) - np.round(VAR_1_stats['VaR_1_1'].loc['mean'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_VaR_quantile_delta = np.round(VAR_1_stats['q_1_0'].loc['mean'] - VAR_1_stats['q_1_1'].loc['mean'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The Average PCVaR Delta between parameter for policies 1 and 2 is: \\n {avg_VaR_delta:,} USD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The Average PCVaR Quantile Delta between parameter for policies 1 and 2 is: \\n {avg_VaR_quantile_delta:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that while the 1-Day PCVaR is greater for policy 1 than for policy 2, meaning more PCV is at risk on any given day at a 95% quantile, the value of this quantile is lower, meaning the PCV has a lower magnitude of negative returns, attesting to the risk mitigating effect of policy 1. Hence, policy 1 is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-day PCV at Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_10_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Over the simulation, for parameters in the sweep corresponding to subset 0,',\n",
    "#       np.round(VAR_10_stats['VaR_10_0'].loc['mean'], 2), 'USD is the mean 10-Day PCVaR at 95% over the 20 runs performed, with associated',\n",
    "#       np.round(VAR_10_stats['q_10_0'].loc['mean'], 4), '% average 5% quantile percentile loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Over the simulation, for parameters in the sweep corresponding to subset 1,',\n",
    "#       np.round(VAR_10_stats['VaR_10_1'].loc['mean'], 2), 'USD is the mean 10-Day PCVaR at 95% over the 20 runs performed, with associated',\n",
    "#       np.round(VAR_10_stats['q_10_1'].loc['mean'], 4), '% average 5% quantile percentile loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"10-day PCV at Risk at 95th quantile for subset 0: \\n {np.round(VAR_10_stats['VaR_10_0'].loc['mean'], 2):,} USD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"10-day PCV at Risk at 95th quantile for subset 1: \\n {np.round(VAR_10_stats['VaR_10_1'].loc['mean'], 2):,} USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = df.groupby(['subset','timestep']).mean().query('subset == 0')\n",
    "stats_df = _df.describe()\n",
    "stats_df.loc['skew'] = _df.skew()\n",
    "stats_df.loc['kurtosis'] = _df.kurtosis()\n",
    "# TODO: max drawdown & other relevant summary stats here\n",
    "\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 4: Capital Allocation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fei_capital_allocation_variables = [\n",
    "    'fei_deposit_idle_balance',\n",
    "    'fei_deposit_liquidity_pool_balance',\n",
    "    'fei_deposit_money_market_balance'\n",
    "]\n",
    "fei_capital_allocation_variables.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.area(df_2.sort_index(), y=fei_capital_allocation_variables, groupnorm='percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allocations = df[fei_capital_allocation_variables].iloc[-1]\n",
    "\n",
    "px.pie(df_allocations.sort_index(), title='FEI Capital Allocation', values=df_allocations.values, names=df_allocations.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assorted Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.plot(y=\"collateralization_ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.plot(y=\"stable_backing_ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.plot(y=\"stable_pcv_ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.plot(y=\"pcv_yield_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2[\"pcv_yield_ratio\"] = df_2[\"pcv_yield\"] / df_2[\"total_user_circulating_fei\"] * 365 / df_2[\"dt\"]\n",
    "\n",
    "df_2.plot(y=\"pcv_yield_ratio\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
