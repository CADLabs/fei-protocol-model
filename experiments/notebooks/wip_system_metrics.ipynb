{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Notebook: System Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Experiment Summary](#Experiment-Summary)\n",
    "* [Experiment Assumptions](#Experiment-Assumptions)\n",
    "* [Experiment Setup](#Experiment-Setup)\n",
    "* [Analysis 1: Sanity Checks](#Analysis-1:-Sanity-Checks)\n",
    "* [Analysis 2: Correlation Matrix](#Analysis-1:-Correlation-Matrix)\n",
    "* [Analysis 3: PCV at Risk](#Analysis-3:-PCV-at-Risk)\n",
    "* [Analysis 4: Capital Allocation Metrics](#Analysis-4:-Capital-Allocation-Metrics)\n",
    "* [Appendix](#Appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Summary \n",
    "\n",
    "The purpose of this notebook is to demonstrate the system's standard metrics, KPIs and goals.\n",
    "\n",
    "# Experiment Assumptions\n",
    "\n",
    "See [assumptions document](../../ASSUMPTIONS.md) for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setup\n",
    "\n",
    "We begin with several experiment-notebook-level preparatory setup operations:\n",
    "\n",
    "* Import relevant dependencies\n",
    "* Import relevant experiment templates\n",
    "* Create copies of experiments\n",
    "* Configure and customize experiments \n",
    "\n",
    "Analysis-specific setup operations are handled in their respective notebook sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the setup module:\n",
    "# * sets up the Python path\n",
    "# * runs shared notebook configuration methods, such as loading IPython modules\n",
    "import setup\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from operator import lt, gt\n",
    "\n",
    "from experiments.notebooks.visualizations.plotly_theme import cadlabs_colorway_sequence\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import itertools\n",
    "\n",
    "import experiments.notebooks.visualizations as visualizations\n",
    "from experiments.run import run\n",
    "from experiments.utils import display_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "png_renderer = pio.renderers[\"png\"]\n",
    "png_renderer.width = 1200\n",
    "png_renderer.height = 500\n",
    "# png_renderer.scale = 1\n",
    "\n",
    "pio.renderers.default = \"png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_cycle = itertools.cycle(cadlabs_colorway_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable/disable logging\n",
    "logger = logging.getLogger()\n",
    "logger.disabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import experiment templates\n",
    "import experiments.default_experiment as default_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simulation for each analysis\n",
    "simulation_0 = copy.deepcopy(default_experiment.experiment.simulations[0])\n",
    "simulation_1 = copy.deepcopy(default_experiment.experiment.simulations[0])\n",
    "simulation_2 = copy.deepcopy(default_experiment.experiment.simulations[0])\n",
    "simulation_3 = copy.deepcopy(default_experiment.experiment.simulations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Analysis Methodology\n",
    "\n",
    "The goal of the FEI Ecosystem Model Risk Analysis is to provide qualitative and quantitative recommendations to surface the most appropriate FEI monetary policy parameter settings across multiple scenarios. The risk analysis includes a cohesive set of model metrics, risk scores and protocol KPIs.\n",
    "\n",
    "The main holistic risk analysis and parameter recommendation tool at our disposal is the FEI Capital Allocation model, where we allow the model to provide risk-weighted target allocations of FEI in all modeled avenues for FEI liquidity - LP, MM, FSD, Idle. These allocation targets will depend on parameter sweeps of FEI's monetary policy levers such as the FEI Savings Rate (see simplified ERD). The highest scoring parameter settings in terms of FEI Capital allocation and associated comparative KPIs will form the quantitative basis for recommendations.\n",
    "\n",
    "It is to be noted that we can analyze KPIs comparatively based on monetary policy parameter settings in <b>Two ways</b> - qualitative/deterministic, and statistically based/stochastic.\n",
    "\n",
    "Since the main driver of volatility in the FEI ecosystem is the Volatile Asset (an abstraction for Ethereum), we can model this volatile asset as Trajectory-based (based on a linear function), or based on a stochastic process such as a Geometric Brownian motion. The application of both settings for the Volatile asset allows us to respectively perform two analysis types.\n",
    "\n",
    "1. Qualitative recommendations can be made as a result of comparative KPI analysis across different monetary policy parameter settings, using a trajectory model for the Volatile Asset. These do NOT involve monte carlo runs. The output of these analyses is to understand the impact on KPIs as a result of monetary policy changes for various final levels of Volatile Asset price. Ex: For a 30% VA price downturn, Stable backing ratio is higher with monetary policy 1 than policy 2, ie: Delta_1,2 Stable Backing ratio > 0, hence policy 1 is recommended.\n",
    "\n",
    "\n",
    "2. Statistically-based recommendations can be made as a result of comparative KPI analysis across different monetary policy parameter settings, using a parameterized stochastic model for the Volatile Asset. These DO involve monte carlo runs. The output of these analyses is to construct a probability distribution for each KPI from which summary statistics can be derived. This allows us to empirically say that for a given parameter setting, KPIs are above or below key thresholds with a certain probability. Ex: Over 1000 simulations, 1-Day PCV at Risk is < 1M USD with a 90% probability with policy 1 and < 1M USD with an 85% probability with policy 2. Additionally, the statistical average PCVaR is lower with policy 1 than with policy 2, ie: Delta_1,2 avg. PCVaR > 0. Hence policy 1 is recommended.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 0: PCV Sanity Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simulation across 4 volatile asset price scenarios to illustrate PCV state evolution. Here, deterministic price trajectories for the Volatile Asset price are used, as opposed to parameterized stochastic processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis-specific setup\n",
    "simulation_0.model.params.update({\n",
    "    \"volatile_asset_price_process\": [\n",
    "        lambda _run, _timestep: 2_000,\n",
    "        lambda _run, timestep: 2_000 if timestep < 365 / 4 else (1_000 if timestep < 365 * 3/4 else 2_000),\n",
    "        lambda _run, timestep: 2_000 * (1 + timestep * 0.2 / 365),\n",
    "        lambda _run, timestep: 2_000 * (1 - timestep * 0.2 / 365),\n",
    "    ],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment execution\n",
    "df_1, exceptions = run(simulation_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Post-processing and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign DataFrame for current analysis\n",
    "df = df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update subset labels\n",
    "scenarios = {0: 'flat_price_trend', 1: 'step_price_trend', 2: 'bullish_price_trend', 3: 'bearish_price_trend'}\n",
    "df['subset_label'] = df['subset'].map(lambda x: scenarios[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df.plot(y='volatile_asset_price', color='subset_label')\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Sanity Check Volatile Asset Price Scenarios\",\n",
    "    xaxis_title=\"Timestamp\",\n",
    "    yaxis_title=\"Volatile Asset Price (USD)\",\n",
    "    autosize=False,\n",
    "    width=1200,\n",
    "    height=675,\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title='Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=5, cols=len(df.subset.unique()), shared_yaxes=True)\n",
    "\n",
    "for subset in df.subset.unique():\n",
    "    df_plot = df.query('subset == @subset')\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.timestamp,\n",
    "            y=df_plot.volatile_asset_price,\n",
    "            name=\"Volatile Asset Price\",\n",
    "            line=dict(color=cadlabs_colorway_sequence[0]),\n",
    "            showlegend=(True if subset == 0 else False),\n",
    "        ),\n",
    "        row=1, col=subset+1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.timestamp,\n",
    "            y=df_plot.total_pcv,\n",
    "            name=\"Total PCV\",\n",
    "            line=dict(color=cadlabs_colorway_sequence[1]),\n",
    "            showlegend=(True if subset == 0 else False),\n",
    "        ),\n",
    "        row=2, col=subset+1\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.timestamp,\n",
    "            y=df_plot.collateralization_ratio,\n",
    "            name=\"Collateralization Ratio\",\n",
    "            line=dict(color=cadlabs_colorway_sequence[2]),\n",
    "            showlegend=(True if subset == 0 else False),\n",
    "        ),\n",
    "        row=3, col=subset+1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.timestamp,\n",
    "            y=df_plot.total_volatile_asset_pcv,\n",
    "            name=\"Volatile Asset PCV\",\n",
    "            line=dict(color=cadlabs_colorway_sequence[3]),\n",
    "            showlegend=(True if subset == 0 else False),\n",
    "            stackgroup='one',\n",
    "        ),\n",
    "        row=4, col=subset+1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.timestamp,\n",
    "            y=df_plot.total_stable_asset_pcv,\n",
    "            name=\"Stable Asset PCV\",\n",
    "            line=dict(color=cadlabs_colorway_sequence[4]),\n",
    "            showlegend=(True if subset == 0 else False),\n",
    "            stackgroup='one',\n",
    "        ),\n",
    "        row=4, col=subset+1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot.timestamp,\n",
    "            y=df_plot.liquidity_pool_tvl,\n",
    "            name=\"Liquidity Pool TVL\",\n",
    "            line=dict(color=cadlabs_colorway_sequence[4]),\n",
    "            showlegend=(True if subset == 0 else False),\n",
    "        ),\n",
    "        row=5, col=subset+1\n",
    "    )\n",
    "\n",
    "\n",
    "fig.update_layout(height=1000, title_text=\"Fei Protocol Model Sanity Checks\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 1: Comparative KPI across Trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis we wish to illustrate the usage of a trajectory-based volatile asset price for comparative analysis of a KPI or statistic of a KPI. In practice, we look at the effect of volatile asset price downturns at four levels: -10%- to -40% - each associated with a target stable PCV ration - on the stable PCV ratio at simulation end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcv_ratio_settings = [0.9, 0.6, 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis-specific setup\n",
    "simulation_1.model.params.update({\n",
    "    \"volatile_asset_price_process\": [\n",
    "        lambda _run, timestep: (2_000 * (1 - timestep * 0.2 / 365)) if timestep < 365 / 4 else ((1_000 * (1 + timestep * 0.2 / 365)) if timestep < 365 * 3/4 else 2_000),\n",
    "        lambda _run, timestep: (2_000 * (1 - timestep * 0.2 / 365)) if timestep < 365 / 4 else ((1_000 * (1 + timestep * 0.2 / 365)) if timestep < 365 * 3/4 else 2_000),\n",
    "        lambda _run, timestep: (2_000 * (1 - timestep * 0.2 / 365)) if timestep < 365 / 4 else ((1_000 * (1 + timestep * 0.2 / 365)) if timestep < 365 * 3/4 else 2_000),\n",
    "    ],\n",
    "    \"target_stable_pcv_ratio\": pcv_ratio_settings,  # Target stable PCV ratio of 20% and 50%\n",
    "    \"target_stable_backing_ratio\": [None],  # Disable stable backing ratio target\n",
    "    \"rebalancing_period\": [int(365 / 4)],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment execution\n",
    "df_1, exceptions = run(simulation_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Post-processing and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign DataFrame for current analysis\n",
    "df = df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update subset labels\n",
    "n_scenarios = 3\n",
    "scenarios = dict(zip(list(range(n_scenarios)), ['trajectory_'+str(x+1) for x in range(n_scenarios)]))\n",
    "policy_settings = dict(zip(list(range(n_scenarios)), ['policy_setting_'+str(x) for x in pcv_ratio_settings]))\n",
    "\n",
    "df['subset_label'] = df['subset'].map(lambda x: scenarios[x])\n",
    "df['scenario_label'] = df['subset'].map(lambda x: policy_settings[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df.plot(y='volatile_asset_price', color='subset_label')\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Volatile Asset Price Trajectories\",\n",
    "    xaxis_title=\"Timestamp\",\n",
    "    yaxis_title=\"Volatile Asset Price (USD)\",\n",
    "    autosize=False,\n",
    "    width=1200,\n",
    "    height=675,\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title='Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = df.plot(y='collateralization_ratio', color='scenario_label')\n",
    "\n",
    "fig2.update_layout(\n",
    "    title=\"Collateralization Ratio\",\n",
    "    xaxis_title=\"Timestamp\",\n",
    "    yaxis_title=\"Collateralization Ratio\",\n",
    "    autosize=False,\n",
    "    width=1200,\n",
    "    height=675,\n",
    ")\n",
    "\n",
    "fig2.update_xaxes(title='Timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference of KPI matrix is constructed by taking the average of the KPI over a given window, and taking the pairwise difference of such KPI averages across trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairwise_rolling_corr(df, variable, window_size):\n",
    "    L = []\n",
    "    k = 0\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if i < j:\n",
    "                d_ = df.query('subset==@i')[variable].rolling(window=window_size).corr(other=df.query('subset==@j')[variable])\n",
    "                d_ = pd.DataFrame(d_)\n",
    "                d_['subset'] = 'policy_setting_'+str(pcv_ratio_settings[i])+'_'+str(pcv_ratio_settings[j])\n",
    "                k += 1\n",
    "                L.append(d_)\n",
    "\n",
    "    return pd.concat(L, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_series_df = generate_pairwise_rolling_corr(df, 'collateralization_ratio', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = corr_series_df.plot(y='collateralization_ratio', color='subset')\n",
    "\n",
    "fig3.update_layout(\n",
    "    title=\"15-Day Rolling Collateralization Ratio Correlation Series\",\n",
    "    xaxis_title=\"Timestamp\",\n",
    "    yaxis_title=\"Collateralization Ratio Pairwise Corrleation\",\n",
    "    autosize=False,\n",
    "    width=1200,\n",
    "    height=675,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_delta_KPI(df, start_ts, end_ts, variable):\n",
    "    \n",
    "    # average of collateral ratio series KPI in set window\n",
    "    avg_kpi = df.query('timestep >= @start_ts and timestep <= @end_ts').groupby('subset')[variable].mean()\n",
    "\n",
    "    # create difference of averages matrix from series\n",
    "    avg_kpi_delta = pd.DataFrame(np.column_stack([avg_kpi, np.subtract.outer(*[-avg_kpi.values]*2)]))\n",
    "    avg_kpi_delta.columns = ['KPI'] + ['delta_KPI_'+str(x) for x in list(policy_settings.values())]\n",
    "    avg_kpi_delta.index = list(scenarios.values())\n",
    "    \n",
    "    z_ = np.array(np.round(avg_kpi_delta.iloc[:,1:],4).to_numpy()),\n",
    "    z__ = z_[0]\n",
    "    z___ = np.triu(z_[0])\n",
    "    z___[z___ == 0] = np.nan\n",
    "\n",
    "    fig = ff.create_annotated_heatmap(\n",
    "            z=z___,\n",
    "            x=avg_kpi_delta.iloc[:,1:].columns.tolist(),\n",
    "            y=avg_kpi_delta.iloc[:,1:].columns.tolist(),\n",
    "            colorscale='rdpu',\n",
    "            zmax=1, zmin=0,\n",
    "            showscale=True,\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Delta Collateral Ratio for each Policy Setting and Price Trajectory at Simulation End\",\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a window further along the simulation, we see the results are qualitatively identical (due to lack of exogenous shocks in the dynamics) but qualitatively more accentuated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ts = 365\n",
    "end_ts = 365\n",
    "\n",
    "display_delta_KPI(df, start_ts, end_ts, 'collateralization_ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the matrix above, generated by looking at the differences in average stable PCV ratio over the first 50 timesteps of the simulation, we see that by looking at the delta_CR columns across trajectory settings, simplistically, the greater the difference in volatile asset price downturn in any two scenarios, the greater the difference in stable PCV ratio. \n",
    "\n",
    "An even simpler conclusion is the average CR over the window is lower for greater levels of ETH downturn, with a difference between trajectories 1 and 4 (-10% and -40% respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 2: Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameter_overrides = {\n",
    "#     \"target_stable_pcv_ratio\": [0.5, 0.7],  # Target stable PCV ratio of 20% and 50%\n",
    "#     \"target_rebalancing_condition\": [lambda a, b: lt(a, b)],  # Rebalance if less than target_stable_pcv_ratio\n",
    "    \"target_rebalancing_condition\": [gt, lt], # Simulate decrease and increase of stable PCV\n",
    "    \"target_stable_pcv_ratio\": [0.2, 0.5],  # Target stable PCV ratio of 20% and 50%\n",
    "    \"target_stable_backing_ratio\": [None],  # Disable stable backing ratio target\n",
    "    #\"rebalancing_period\": [int(365 / 4), int(365 / 4), int(365 / 12), int(365 / 12)],  # Rebalance quarterly vs monthly\n",
    "}\n",
    "\n",
    "# Analysis-specific setup\n",
    "simulation_2.model.params.update(parameter_overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment execution\n",
    "df_2, exceptions = run(simulation_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign DataFrame for current analysis\n",
    "df = df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.round(df.query('subset == 0')[variables].corr().values,2).astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    \"total_stable_asset_pcv\",\n",
    "    \"total_pcv\",\n",
    "    \"collateralization_ratio\",\n",
    "    \"total_user_circulating_fei\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = df.query('subset == 0')[variables].corr()\n",
    "\n",
    "fig = ff.create_annotated_heatmap(\n",
    "            z=np.round(z1,2).to_numpy(),\n",
    "            x=z1.columns.tolist(),\n",
    "            y=z1.index.tolist(),\n",
    "            colorscale='rdpu',\n",
    "            zmax=1, zmin=-0.5,\n",
    "            showscale=True,\n",
    "            )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Correlations for Sanity Check State Variables - Policy 1\",\n",
    "#     xaxis_title=\"Timestamp\",\n",
    "#     yaxis_title=\"Volatile Asset Price (USD)\",\n",
    "    autosize=False,\n",
    "    width=1200,\n",
    "    height=675,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2 = df.query('subset == 1')[variables].corr()\n",
    "\n",
    "fig2 = ff.create_annotated_heatmap(\n",
    "            z=np.round(z2,2).to_numpy(),\n",
    "            x=z2.columns.tolist(),\n",
    "            y=z2.index.tolist(),\n",
    "            colorscale='rdpu',\n",
    "            zmax=1, zmin=-1,\n",
    "            showscale=True,\n",
    "            )\n",
    "\n",
    "fig2.update_layout(\n",
    "    title=\"Correlations for Sanity Check State Variables - Policy 2\",\n",
    "#     xaxis_title=\"Timestamp\",\n",
    "#     yaxis_title=\"Volatile Asset Price (USD)\",\n",
    "    autosize=False,\n",
    "    width=1200,\n",
    "    height=675,\n",
    ")\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_states_vs_volatile_asset_price(df):\n",
    "    fig_1 = px.scatter(df, x='volatile_asset_price', y='total_pcv', color='timestep')\n",
    "    fig_1.update_layout(\n",
    "        title=\"PCV vs. Volatile Asset Price\",\n",
    "        autosize=False,\n",
    "        width=800,\n",
    "        height=800,\n",
    "    )\n",
    "    fig_1.update_xaxes(title=\"Volatile Asset Price (USD)\")\n",
    "    fig_1.update_yaxes(title=\"PCV (USD)\")\n",
    "    fig_1.show()\n",
    "    \n",
    "    fig_2 = px.scatter(df, x='volatile_asset_price', y='collateralization_ratio_pct', color='timestep')\n",
    "    fig_2.update_layout(\n",
    "        title=\"Collateralization Ratio vs. Volatile Asset Price\",\n",
    "        autosize=False,\n",
    "        width=800,\n",
    "        height=800,\n",
    "    )\n",
    "    fig_2.update_xaxes(title=\"Volatile Asset Price (USD)\")\n",
    "    fig_2.update_yaxes(title=\"Collateralization Ratio (%)\")\n",
    "    fig_2.show()\n",
    "    \n",
    "    fig_3 = px.scatter(df, x='volatile_asset_price', y='total_user_circulating_fei', color='timestep')\n",
    "    fig_3.update_layout(\n",
    "        title=\"User-circulating FEI vs. Volatile Asset Price\",\n",
    "        autosize=False,\n",
    "        width=800,\n",
    "        height=800,\n",
    "    )\n",
    "    fig_3.update_xaxes(title=\"Volatile Asset Price (USD)\")\n",
    "    fig_3.update_yaxes(title=\"User-circulating FEI (USD)\")\n",
    "    fig_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_states_vs_volatile_asset_price(df.query('subset == 0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_states_vs_volatile_asset_price(df.query('subset == 1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 3: PCV at Risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Summary\n",
    "\n",
    "In this analysis, which is based on a stochastic Volatile Asset Price process with 10 realizations (10 monte carlo runs), we look at two policies (a parameter sweep of size 2) and look at the empirical probability of the PCV at Risk KPI being below a certain threshold, as well as comparatively examine the value of the KPI across both policies to yield a recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.stochastic_processes import generate_volatile_asset_price_scenarios\n",
    "df_price_scenarios = generate_volatile_asset_price_scenarios()\n",
    "df_price_scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df_price_scenarios.plot(y=['base_price_trend', 'bearish_price_trend', 'bullish_price_trend'])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Stochastic Volatile Asset Price Trend Scenarios\",\n",
    "    xaxis_title=\"Timestamp\",\n",
    "    yaxis_title=\"Volatile Asset Price (USD)\",\n",
    "    autosize=False,\n",
    "    width=1200,\n",
    "    height=675,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df_price_scenarios.plot(y=['base_price_volatility', 'low_price_volatility', 'high_price_volatility'])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Stochastic Volatile Asset Price Volatility Scenarios\",\n",
    "    xaxis_title=\"Timestamp\",\n",
    "    yaxis_title=\"Volatile Asset Price (USD)\",\n",
    "    autosize=False,\n",
    "    width=1200,\n",
    "    height=675,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute N Monte Carlo runs\n",
    "simulation_3.runs = 6\n",
    "\n",
    "# Construct Monte Carlo simulation of all stochastic price scenarios\n",
    "volatile_asset_price_samples = [scenario for _key, scenario in df_price_scenarios.iteritems()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_overrides = {\n",
    "#     \"volatile_asset_price_process\": [\n",
    "#         lambda run, timestep: volatile_asset_price_samples[run - 1][timestep]\n",
    "#     ],  # Perform Monte Carlo simulation of all stochastic price scenarios above\n",
    "    \"target_rebalancing_condition\": [gt, lt], # Simulate decrease and increase of stable PCV\n",
    "    \"target_stable_pcv_ratio\": [0.2, 0.5],  # Target stable PCV ratio of 20% and 50%\n",
    "    \"target_stable_backing_ratio\": [None],  # Disable stable backing ratio target\n",
    "    #\"target_rebalancing_condition\": [lambda a, b: lt(a, b), lambda a, b: gt(a, b)],  # Rebalance if less than target_stable_pcv_ratio\n",
    "    #\"rebalancing_period\": [int(365 / 4), int(365 / 4), int(365 / 12), int(365 / 12)],  # Rebalance quarterly vs monthly\n",
    "}\n",
    "\n",
    "# Update default model parameters and construct parameter sweep / parameter subsets\n",
    "simulation_3.model.params.update(parameter_overrides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment execution\n",
    "df_3, exceptions = run(simulation_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3['subset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.groupby([\n",
    "    'subset'\n",
    "])[[\n",
    "    'target_stable_pcv_ratio',\n",
    "    'rebalancing_period',\n",
    "]].last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update run labels\n",
    "scenarios = dict(enumerate(df_price_scenarios))\n",
    "df_3['run_label'] = df_3['run'].map(lambda x: scenarios[x - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.query('subset == 0').plot(y=\"volatile_asset_price\", color=\"run_label\", labels=dict(value=df_price_scenarios.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.query('subset == 0').plot(y=\"total_pcv\", color=\"run_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.query(\"run_label == 'bearish_price_trend'\").plot(y=\"stable_pcv_ratio\", color='subset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.query(\"run_label == 'bullish_price_trend'\").plot(x='timestep', y=\"stable_pcv_ratio\", color='subset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCV at Risk Calculation Across All Subsets and Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_VaR(df, state_variable, alpha, timesteps):\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    for simulation in df.simulation.unique():\n",
    "        df_simulation = df.query(\"simulation == @simulation\")\n",
    "        for subset in df_simulation.subset.unique():\n",
    "            df_subset = df_simulation.query(\"subset == @subset\")\n",
    "            for run in df_subset.run.unique():\n",
    "                df_run = df_subset.query(\"run == @run\")\n",
    "\n",
    "                returns = df_run[state_variable].pct_change()\n",
    "                final_value = df_run[state_variable].iloc[-1]\n",
    "                q = returns.quantile(1 - alpha)\n",
    "                value_at_risk = abs(final_value * q) * np.sqrt(timesteps)\n",
    "\n",
    "                result = pd.DataFrame({'simulation': [simulation], 'subset': [subset], 'run': [run], 'VaR': [value_at_risk], 'q': [q]})\n",
    "                results = pd.concat([results, result])\n",
    "\n",
    "    return results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var = calculate_VaR(df_3, \"total_pcv\", alpha=0.95, timesteps=1)\n",
    "df_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var.query(\"subset == 0\")[[\"VaR\", \"q\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var_stats_0 = df_var.query(\"subset == 0\")[[\"VaR\", \"q\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"1-day average PCV at Risk at 95th quantile for subset 0: \\n {df_var_stats_0['VaR'].loc['mean']:,.2f} USD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var_stats_1 = df_var.query(\"subset == 1\")[[\"VaR\", \"q\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"1-day average PCV at Risk at 95th quantile for subset 1: \\n {df_var_stats_1['VaR'].loc['mean']:,.2f} USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-day PCV at Risk for Policy 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visualization of PCVaR calculation for one monte carlo run for one Policy (parameter setting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_to_plot(df, run, subset):\n",
    "    pcv_ret = df.query('run == @run and subset == @subset')['total_pcv'].pct_change()\n",
    "    var = df_var.query('run == @run and subset == @subset')['VaR'].iloc[0]\n",
    "    q = df_var.query('run == @run and subset == @subset')['q'].iloc[0]\n",
    "    \n",
    "    return pcv_ret, var, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=6, cols=2,\n",
    "                    x_title='PCV Daily Returns - Left: Policy 0, Right: Policy 1',\n",
    "                    y_title='Number of Observations',\n",
    "                   )\n",
    "\n",
    "for subset in [0, 1]:\n",
    "    for run in range(1,7):\n",
    "\n",
    "        pcv_ret, var, q = get_data_to_plot(df_3, run, subset)\n",
    "\n",
    "        fig.add_trace(\n",
    "            px.histogram(pcv_ret, x=\"total_pcv\", nbins=100).data[0],\n",
    "            row=run, col=subset+1)\n",
    "\n",
    "        fig.add_vline(x=q, row=run, col=subset+1)\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Histogram of PCV Returns for Runs and Policy Settings\",\n",
    "    autosize=False,\n",
    "    #width=1200,\n",
    "    height=1600,\n",
    ")\n",
    "\n",
    "#fig.update_xaxes(xaxis_title='a')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f'1-Day PCVar for Run 1, Policy 0 (Subset 0) is {var:,.2f} USD with 5% quantile value {100*q:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of PCV at Risk Being Below Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_return_threshold = -0.01\n",
    "\n",
    "\n",
    "def calculate_VaR_threshold_probability(df, threshold):\n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    for subset in df.subset.unique():\n",
    "        df_subset = df.query(\"subset == @subset\")\n",
    "        \n",
    "        df_threshold = df_subset[\"q\"] <= threshold\n",
    "        probability = df_threshold.sum() / len(df_threshold)\n",
    "        \n",
    "        result = pd.DataFrame({'subset': [subset], 'threshold': [threshold], 'probability': [probability]})\n",
    "        results = pd.concat([results, result])\n",
    "    \n",
    "    return results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_probabilities = calculate_VaR_threshold_probability(df_var, threshold=quantile_return_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in q_probabilities.subset.unique():\n",
    "    print(f\"\"\"For Policy {subset + 1}, the 1-Day PCV at Risk is less than {abs(quantile_return_threshold*100):.2f}% with a {100*q_probabilities.query('subset == @subset')['probability'].iloc[0]:.2f}% probability\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this analysis, we see that over the 10 monte carlo runs for each policy (each subset), since the probability of PCV at risk being less than 2% of total PCV on any given day is lower for policy 1 than for policy 2, policy 1 does a better job at risk mitigation, hence we recommend policy 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative PCV at Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_VaR_delta = df_var_stats_0['VaR'].loc['mean'] - df_var_stats_1['VaR'].loc['mean']\n",
    "avg_VaR_quantile_delta = df_var_stats_0['q'].loc['mean'] - df_var_stats_1['q'].loc['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The Average PCVaR Delta between parameter for policies 1 and 2 is: \\n {avg_VaR_delta:,.2f} USD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The Average PCVaR Quantile Delta between parameter for policies 1 and 2 is: \\n {avg_VaR_quantile_delta:,.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that while the 1-Day PCVaR is greater for policy 1 than for policy 2, meaning more PCV is at risk on any given day at a 95% quantile, the value of this quantile is lower, meaning the PCV has a lower magnitude of negative returns, attesting to the risk mitigating effect of policy 1. Hence, policy 1 is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 4: Capital Allocation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fei_capital_allocation_variables = [\n",
    "    'fei_idle_pcv_deposit_balance',\n",
    "    'fei_liquidity_pool_pcv_deposit_balance',\n",
    "    'fei_money_market_pcv_deposit_balance'\n",
    "]\n",
    "fei_capital_allocation_variables.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.area(df_2, y=fei_capital_allocation_variables, groupnorm='percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allocations = df[fei_capital_allocation_variables].iloc[-1]\n",
    "\n",
    "px.pie(df_allocations.sort_index(), values=df_allocations.values, names=df_allocations.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign DataFrame for current analysis\n",
    "df = df_3.query('run == 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = df.groupby(['subset','timestep']).mean().query('subset == 0')\n",
    "stats_df = _df.describe()\n",
    "stats_df.loc['skew'] = _df.skew()\n",
    "stats_df.loc['kurtosis'] = _df.kurtosis()\n",
    "# TODO: max drawdown & other relevant summary stats here\n",
    "\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df.plot(y=\"collateralization_ratio_pct\", color='subset')\n",
    "fig.add_hline(y=150)\n",
    "fig.update_layout(\n",
    "    title=\"Collateralization Ratio Over Time\",\n",
    "    xaxis_title=\"Timestamp\",\n",
    "    yaxis_title=\"Collateralization Ratio (%)\",\n",
    "    autosize=False,\n",
    "    width=1200,\n",
    "    height=675,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df.plot(y=\"stable_backing_ratio_pct\", color='subset')\n",
    "fig.add_hline(y=70)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df.plot(y=\"stable_pcv_ratio_pct\", color='subset')\n",
    "fig.add_hline(y=50)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df.plot(y=\"pcv_yield_rate_pct\", color='subset')\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"PCV Yield Rate\",\n",
    ")\n",
    "fig.update_xaxes(title=\"Timestamp\")\n",
    "fig.update_yaxes(title=\"PCV Yield Rate (%)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df.plot(y=\"pcv_yield_ratio_pct\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"PCV Yield Ratio\",\n",
    ")\n",
    "fig.update_xaxes(title=\"Timestamp\")\n",
    "fig.update_yaxes(title=\"PCV Yield Ratio (%)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df.plot(y=\"protocol_equity\", color='subset')\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Protocol Equity\",\n",
    ")\n",
    "fig.update_xaxes(title=\"Timestamp\")\n",
    "fig.update_yaxes(title=\"Protocol Equity (USD)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CADLabs Fei Model)",
   "language": "python",
   "name": "python-cadlabs-fei"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
